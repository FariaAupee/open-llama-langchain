{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irfn5iWBjEN_",
        "outputId": "fcb818a9-7525-44c0-8837-27f2bc257160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU4Khx-qThrb",
        "outputId": "518e5ea0-7922-4e59-e311-c519f3bcd455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n",
            "Requirement already satisfied: appdirs==1.4.4 in /usr/local/lib/python3.10/dist-packages (1.4.4)\n",
            "Collecting bitsandbytes==0.37.2\n",
            "  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.37.2\n",
            "Collecting datasets==2.10.1\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (4.65.0)\n",
            "Collecting xxhash (from datasets==2.10.1)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.10.1)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0 (from datasets==2.10.1)\n",
            "  Downloading huggingface_hub-0.16.3-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (23.1)\n",
            "Collecting responses<0.19 (from datasets==2.10.1)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.10.1 dill-0.3.6 huggingface-hub-0.16.3 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
            "Collecting fire==0.5.0\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire==0.5.0) (2.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=13e21c124ed8c9dbc5b9fb636aeb7946710846c4edb3fcbb0cc61ee07e1b3ee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.5.0\n",
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-5efl1bg8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-5efl1bg8\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 39ef2546d5d9b8f5f8a7016ec10657887a867041\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n",
            "Collecting transformers (from peft==0.4.0.dev0)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (0.20.3)\n",
            "Collecting safetensors (from peft==0.4.0.dev0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.16.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->peft==0.4.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=62147 sha256=3721cc21aab086db4b3c7d09478474bc9c3843de59b0fe13aeabde7e04aefb4d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-495sg5xq/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "Successfully built peft\n",
            "Installing collected packages: tokenizers, safetensors, transformers, peft\n",
            "Successfully installed peft-0.4.0.dev0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-9p45gzlc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-9p45gzlc\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 495729427045c7a58e040fa9bf6df81c16f54208\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (0.16.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7286095 sha256=bc1fb0688ec4c537cf2b15d947c2bdfc88ef56d353f16ab1e6767537b0b072a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tjnduvry/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.2\n",
            "    Uninstalling transformers-4.30.2:\n",
            "      Successfully uninstalled transformers-4.30.2\n",
            "Successfully installed transformers-4.31.0.dev0\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.40.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Collecting tensorboardX==2.6\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.6) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.6) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.6) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n",
            "Collecting gradio==3.23.0\n",
            "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from gradio==3.23.0)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.23.0)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.23.0)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (2023.6.0)\n",
            "Collecting httpx (from gradio==3.23.0)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (0.16.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23.0)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (1.22.4)\n",
            "Collecting orjson (from gradio==3.23.0)\n",
            "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (1.10.9)\n",
            "Collecting pydub (from gradio==3.23.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.23.0)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (2.27.1)\n",
            "Collecting semantic-version (from gradio==3.23.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio==3.23.0) (4.6.3)\n",
            "Collecting uvicorn (from gradio==3.23.0)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio==3.23.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.23.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.23.0) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.23.0) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->gradio==3.23.0) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->gradio==3.23.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->gradio==3.23.0) (23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.23.0)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.23.0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.23.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.23.0) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.23.0) (1.3.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.23.0)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.23.0) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio==3.23.0)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.23.0) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.23.0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.23.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.23.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.23.0) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.23.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.23.0) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.23.0) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->gradio==3.23.0) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn->gradio==3.23.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.23.0) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23.0) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.23.0)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio==3.23.0) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.23.0) (1.1.1)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=62c917c952d5564881396581e516e2356b92f5afdce8a2f3a206aee9e746c057\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, httpcore, httpx, fastapi, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.99.1 ffmpy-0.3.0 gradio-3.23.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.1 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip\n",
        "!pip install accelerate -U\n",
        "!pip install appdirs==1.4.4\n",
        "!pip install bitsandbytes==0.37.2\n",
        "!pip install datasets==2.10.1\n",
        "!pip install fire==0.5.0\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git -U\n",
        "!pip install torch==2.0.0\n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install tensorboardX==2.6\n",
        "!pip install gradio==3.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yiiDs5nO9qJ",
        "outputId": "314b6dac-99de-409b-e0ac-b21a1ee46760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/peft@27af2198225cbb9e049f548440f2bd0fba2204aa\n",
            "  Cloning https://github.com/huggingface/peft (to revision 27af2198225cbb9e049f548440f2bd0fba2204aa) to /tmp/pip-req-build-s081eary\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-s081eary\n",
            "  Running command git rev-parse -q --verify 'sha^27af2198225cbb9e049f548440f2bd0fba2204aa'\n",
            "  Running command git fetch -q https://github.com/huggingface/peft 27af2198225cbb9e049f548440f2bd0fba2204aa\n",
            "  Running command git checkout -q 27af2198225cbb9e049f548440f2bd0fba2204aa\n",
            "  Resolved https://github.com/huggingface/peft to commit 27af2198225cbb9e049f548440f2bd0fba2204aa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=56159 sha256=8709ba5ba4e55425c95e86d87956bd24954a2104c3e87011365f484aaff466ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/a2/e4/da7f5b06015c96102ac662eab4ee8fb2eeffdbc79e8a57f052\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.4.0.dev0\n",
            "    Uninstalling peft-0.4.0.dev0:\n",
            "      Successfully uninstalled peft-0.4.0.dev0\n",
            "Successfully installed peft-0.4.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/peft@27af2198225cbb9e049f548440f2bd0fba2204aa --force-reinstall --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BtcsO1YdlA3K",
        "outputId": "0bf2678b-ff5e-47be-c61a-6b94510d646f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1kz4ixzf36wk0 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "import textwrap\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "import os\n",
        "import sys\n",
        "from typing import List\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        ")\n",
        "\n",
        "import fire\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(rc={'figure.figsize':(10, 7)})\n",
        "sns.set(rc={'figure.dpi':100})\n",
        "sns.set(style='white', palette='muted', font_scale=1.2)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SVF6yKZJlt8F"
      },
      "outputs": [],
      "source": [
        "# # import locale\n",
        "# # locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# !gdown 1xQ89cpZCnafsW5T3G3ZQWvR7q682t2BN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-7itwOltlweg"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"bitcoin-sentiment-tweets.csv\")\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsUpaSiYoUly"
      },
      "outputs": [],
      "source": [
        "# df.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ6fArVsoaep"
      },
      "outputs": [],
      "source": [
        "# def sentiment_score_to_name(score: float):\n",
        "#     if score > 0:\n",
        "#         return \"Positive\"\n",
        "#     elif score < 0:\n",
        "#         return \"Negative\"\n",
        "#     return \"Neutral\"\n",
        "\n",
        "# dataset_data = [\n",
        "#     {\n",
        "#         \"instruction\": \"Detect the sentiment of the tweet.\",\n",
        "#         \"input\": row_dict[\"tweet\"],\n",
        "#         \"output\": sentiment_score_to_name(row_dict[\"sentiment\"])\n",
        "#     }\n",
        "#     for row_dict in df.to_dict(orient=\"records\")\n",
        "# ]\n",
        "\n",
        "# dataset_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InOaziDJolI1"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "tokenizer.pad_token_id = (\n",
        "    0  # unk. we want this to be different from the eos token\n",
        ")\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x0qhPTTCD8T"
      },
      "outputs": [],
      "source": [
        "# model.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyaF1pPN3Rql"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# # model.generate()\n",
        "# # Path to the input and output files\n",
        "# input_file_path = \"/content/MyDrive/MyDrive/llama/ethical-wear.txt\"\n",
        "# output_file_path = \"/content/MyDrive/MyDrive/llama/ethicalwear.txt\"\n",
        "\n",
        "# # Read the input file\n",
        "# with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "#     content = input_file.read()\n",
        "\n",
        "# # Add a new line after each full stop\n",
        "# modified_content = content.replace('. ', '.\\n')\n",
        "\n",
        "# # Write the modified content to the output file\n",
        "# with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "#     output_file.write(modified_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk-ucCZVpuaP"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"json\", data_files=\"/content/MyDrive/MyDrive/llama/instructions.json\")\n",
        "data[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGUSl6mYpxuY"
      },
      "outputs": [],
      "source": [
        "# Dataset({\n",
        "#     features: ['instruction', 'input', 'output'],\n",
        "#     num_rows: 1897\n",
        "# })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc-7zDYRshm0"
      },
      "outputs": [],
      "source": [
        "CUTOFF_LEN = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBT3qZHirEhu"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"Below is an instruction that describes a task, paired with an input. Write a response that appropriately completes the request. give precisely the answer as the instruction asks. Do not generate further instruction.\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "### Response:\n",
        "{data_point[\"output\"]+tokenizer.eos_token}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(prompt, add_eos_token=True):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    if (\n",
        "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
        "        and add_eos_token\n",
        "    ):\n",
        "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "        result[\"attention_mask\"].append(1)\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenize(full_prompt)\n",
        "    return tokenized_full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "4a045c6b29804b66b58e06fcdfe3cb18",
            "1c2af134099f45bab077ccfd461b8f31",
            "ff68e9fdc6994d06ba6eccc27ef9facb",
            "792bb58c57b846d3a20243ab349eefd3",
            "5b13f3fd744d4e698f8769864b5c1a44",
            "08a7fae4d4604dbd8b387f89e3f6f7d4",
            "f172f02fe79e42f5b74a135493861bc6",
            "86179f9e043e40ed8048686b40b2ff85",
            "af6362da7503492db08aa174f6478e38",
            "b4f8c9d7a7c4467da5484feb56764ad1",
            "2de6c494dd954beaaae9b15d319e271c"
          ]
        },
        "id": "3uFbyiaArGdb",
        "outputId": "1ac286ef-74c4-4e13-8112-79b6493b5b99"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a045c6b29804b66b58e06fcdfe3cb18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/284 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 284\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data = (\n",
        "    data[\"train\"].map(generate_and_tokenize_prompt)\n",
        ")\n",
        "# train_data = data.map(lambda samples: tokenizer(samples['instruction'],samples['output']), batched=True)\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm3PqBNvDf5f"
      },
      "outputs": [],
      "source": [
        "# train_data['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXiAgfHYssbj"
      },
      "outputs": [],
      "source": [
        "LORA_R = 8\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT= 0.05\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\",\n",
        "]\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "MICRO_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "LEARNING_RATE = 3e-4\n",
        "TRAIN_STEPS = 200\n",
        "OUTPUT_DIR = \"/content/MyDrive/MyDrive/weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RE_q7zMtKOX",
        "outputId": "531cc582-c21b-4f38-a503-0cbadc36b298"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 6,291,456 || all params: 6,744,707,072 || trainable%: 0.09327989982127426\n"
          ]
        }
      ],
      "source": [
        "model1 = prepare_model_for_int8_training(model)\n",
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model1 = get_peft_model(model1, config)\n",
        "model1.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp41ao65tUqm"
      },
      "outputs": [],
      "source": [
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=30,\n",
        "    max_steps=TRAIN_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_torch\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        "    # eval_steps=50,\n",
        "    save_steps=50,\n",
        "    output_dir='OUTPUT_DIR',\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCIvJ4SZwO-4"
      },
      "outputs": [],
      "source": [
        "data_collator = transformers.DataCollatorForSeq2Seq(\n",
        "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        ")\n",
        "# data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2dBnFqvwdW2"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model1,\n",
        "    train_dataset=train_data,\n",
        "    # eval_dataset=val_data,\n",
        "    args=training_arguments,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "model1.config.use_cache = False\n",
        "# old_state_dict = model.state_dict\n",
        "# model.state_dict = (\n",
        "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
        "#         self, old_state_dict()\n",
        "#     )\n",
        "# ).__get__(model, type(model))\n",
        "\n",
        "model1 = torch.compile(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "LV5LFpSCECEC",
        "outputId": "9ea7d16e-68c4-489f-a594-35802a94e821"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 06:15, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.077200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.981500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.242900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.347300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.918200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.049900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.926700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.836900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.913600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.962100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.878500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.805300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.878200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.757300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.736200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.598600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=1.1624943351745605, metrics={'train_runtime': 381.8249, 'train_samples_per_second': 0.524, 'train_steps_per_second': 0.524, 'total_flos': 1030457253298176.0, 'train_loss': 1.1624943351745605, 'epoch': 0.7})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k48b0qHMDCC0",
        "outputId": "8bd59921-66b1-419e-e023-1b46d331867a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/MyDrive/MyDrive/weights/tokenizer_config.json',\n",
              " '/content/MyDrive/MyDrive/weights/special_tokens_map.json',\n",
              " '/content/MyDrive/MyDrive/weights/tokenizer.model',\n",
              " '/content/MyDrive/MyDrive/weights/added_tokens.json')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.save_pretrained(OUTPUT_DIR)\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "# trainer.save_tokenizer(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qK3UVOVbr1-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKJYurjnbGjm",
        "outputId": "9502fe07-14a5-4c3d-b58e-88db636f7fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=50,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8seGFmQbo7u",
        "outputId": "ab74681b-60ce-4cf4-c63f-da110bf86e16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1264: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        }
      ],
      "source": [
        "text = \"What sizes are available for 1 People's collection?\"\n",
        "device = \"cuda:0\"\n",
        "raw_output = pipe(text)\n",
        "print(raw_output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9V6Jtr7C8V2",
        "outputId": "b6b38c9e-8059-4860-9f21-69c77148ddfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "different things about PACT fashion brand PACT is a sustainable fashion brand that is committed to creating high-quality, ethical clothing. They use organic cotton, recycled polyester, and other eco-friendly materials. They also prioritize fair labor practices and trace their supply chain. PACT is a great choice for conscious consumers looking for ethical and sustainable clothing. They are a B-rated brand overall. PACT is a sustainable fashion brand that\n"
          ]
        }
      ],
      "source": [
        "text = \"different things about PACT fashion brand\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=100,temperature=0.5)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMbHL8thMQR"
      },
      "outputs": [],
      "source": [
        "# text = \"different things about PACT?\"\n",
        "# device = \"cuda:0\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "# outputs = model.generate(**inputs, max_new_tokens=100,temperature=0.5)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8HYeo04_93c"
      },
      "outputs": [],
      "source": [
        "# text = \"different things about PACT fashion brand\"\n",
        "# device = \"cuda:0\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens=100,temperature=0.1 )\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6Yz6le19z-"
      },
      "outputs": [],
      "source": [
        "# text = \"what is tentree\"\n",
        "# device = \"cuda:0\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ysyLc0aFRmd"
      },
      "outputs": [],
      "source": [
        "# text = \"Why we use tentree?\"\n",
        "# device = \"cuda:0\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.5)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_3mDeSbFoL7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8mJVeZLO9Uh"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import gc\n",
        "# config = None\n",
        "# model = None\n",
        "# tokenizer=None\n",
        "# trainer=None\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRPQMU7RGu04"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\" #token for personal email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm-BtP4XJics"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\" #token for personal email 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzk3qPe_-FhV"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\" #-_-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnLYixWlJJKS"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-23BBtYKLeu"
      },
      "outputs": [],
      "source": [
        "# model.push_to_hub(\"haraygese/llama-7b-for-custom-data\", use_auth_token=True, create_pr=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ9ZMwXKM9xh"
      },
      "outputs": [],
      "source": [
        "# tokenizer.push_to_hub(\"haraygese/llama-7b-for-custom-data\", use_auth_token=True, create_pr=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "8ebe3454387f4aa5a81b13316b65c62c",
            "0676264d73554a53a69aa586c9451b81",
            "e8eba6c1a6064b41b197e69fbd080a0b",
            "0c1ca9b2b5c64ac89240bfe40403123a",
            "9c6a551642be4b0ebce3a8646cd12921",
            "f1a08049d3a04ffc940574dbbdceda93",
            "d85fb18568e3470e97e2fba40a0054a0",
            "905583213b0f438aa0c225e5d8767445",
            "0cc3f46b77784e7d891c34594bfbc74b",
            "12aee97766494b0b899bcbf7c2959253",
            "f2d0b36eb8ef4037a28375829c1649b0"
          ]
        },
        "id": "-14Z8SBJ62-l",
        "outputId": "d2f2da0d-23d3-4d33-fbc4-25d512cb1ef8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ebe3454387f4aa5a81b13316b65c62c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "# # pip install peft transformers\n",
        "# import torch\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "# from accelerate import infer_auto_device_map, init_empty_weights\n",
        "\n",
        "# peft_model_id = OUTPUT_DIR\n",
        "\n",
        "# config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "# model1 = LlamaForCausalLM.from_pretrained(\n",
        "#     config.base_model_name_or_path,\n",
        "#     torch_dtype='auto',\n",
        "#     device_map='auto',\n",
        "#     offload_folder=\"offload\", offload_state_dict = True\n",
        "# )\n",
        "# tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# # Load the Lora model\n",
        "# model1 = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvjxkqIC_pVm",
        "outputId": "32b8031d-2527-438c-d66a-8a3616d24a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PACT fashion brand PAC PAC PAC PAC PAC\n"
          ]
        }
      ],
      "source": [
        "# text = \"PACT fashion brand\"\n",
        "# device = \"cuda:0\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# outputs = model1.generate(**inputs, max_new_tokens=10, temperature=0.5)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYph7BpaULEw"
      },
      "outputs": [],
      "source": [
        "# from collections import deque\n",
        "\n",
        "# # Load the fine-tuned LLM model\n",
        "# # model = LlamaForCausalLM.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# # Initialize a rolling buffer to store recent user inputs and generated responses\n",
        "# rolling_buffer = deque(maxlen=50)  # Adjust the maximum number of entries as per your requirements\n",
        "\n",
        "# while True:\n",
        "#     # Take user input\n",
        "#     user_input = input(\"User input: \")\n",
        "\n",
        "#     # Check for exit condition\n",
        "#     if user_input.lower() == \"exit\":\n",
        "#         break\n",
        "\n",
        "#     # Tokenize and encode user input\n",
        "#     encoded_user_input = tokenizer(user_input, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "#     # Generate a response\n",
        "#     generated_output = model.generate(**encoded_user_input, max_new_tokens=20)\n",
        "#     response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "\n",
        "#     # Append user input and response to the rolling buffer\n",
        "#     rolling_buffer.append((user_input, response))\n",
        "\n",
        "#     # Fine-tune the model periodically based on the rolling buffer\n",
        "#     if len(rolling_buffer) >= 10:  # Set your desired threshold here\n",
        "#         # Create a new training dataset from the rolling buffer\n",
        "#         new_train_data = []\n",
        "#         for example in rolling_buffer:\n",
        "#             new_train_data.append(example[0] + \" \" + example[1])\n",
        "#         encoded_new_train_data = tokenizer(new_train_data, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "#         # Fine-tune the model with the new training dataset\n",
        "#         trainer = transformers.Trainer(\n",
        "#             model=model,\n",
        "#             train_dataset=encoded_new_train_data,\n",
        "#             eval_dataset=val_data,\n",
        "#             args=training_arguments,\n",
        "#             data_collator=data_collator\n",
        "#         )\n",
        "#         trainer.train()\n",
        "\n",
        "#     # Continue the conversation with the user\n",
        "#     print(\"Chatbot response:\", response)\n",
        "\n",
        "# # Save the final updated model\n",
        "# model.save_pretrained(OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M04WoH2ptRKf"
      },
      "outputs": [],
      "source": [
        "# from collections import deque\n",
        "\n",
        "\n",
        "# def collect_feedback():\n",
        "#     feedback = input(f\"Was the response helpful? (yes/no/skip): \")\n",
        "#     if feedback.lower() == \"yes\":\n",
        "#         return '1'\n",
        "#     elif feedback.lower() == \"no\":\n",
        "#         return '-1'\n",
        "#     else:\n",
        "#         return None\n",
        "# # Load the fine-tuned LLM model\n",
        "# # model = LlamaForCausalLM.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# # Initialize a rolling buffer to store recent user inputs and generated responses\n",
        "# rolling_buffer = deque(maxlen=50)  # Adjust the maximum number of entries as per your requirements\n",
        "\n",
        "# while True:\n",
        "#     # Take user input\n",
        "#     user_input = input(\"User input: \")\n",
        "\n",
        "#     # Check for exit condition\n",
        "#     if user_input.lower() == \"exit\":\n",
        "#         break\n",
        "\n",
        "#     # Tokenize and encode user input\n",
        "#     encoded_user_input = tokenizer(user_input, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "#     # Generate a response\n",
        "#     generated_output = model.generate(**encoded_user_input, max_new_tokens=20)\n",
        "#     response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "#     response = response.replace(user_input,\"\")\n",
        "#     # Continue the conversation with the user\n",
        "#     print(\"Chatbot response:\", response)\n",
        "\n",
        "#     feedback = collect_feedback()\n",
        "#     if feedback is not None:\n",
        "#       # Append user input and response to the rolling buffer\n",
        "#       rolling_buffer.append((user_input, response, feedback))\n",
        "#       if len(rolling_buffer) >= 10:  # Set your desired threshold here\n",
        "#         # Create a new training dataset from the rolling buffer\n",
        "#         new_train_data = []\n",
        "#         for example in rolling_buffer:\n",
        "#             new_train_data.append(example[0] + \" \" + example[1]+\" \"+example[2])\n",
        "#         encoded_new_train_data = tokenizer(new_train_data, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "#         # Fine-tune the model with the new training dataset\n",
        "#         trainer = transformers.Trainer(\n",
        "#             model=model,\n",
        "#             train_dataset=encoded_new_train_data,\n",
        "#             # eval_dataset=val_data,\n",
        "#             args=training_arguments,\n",
        "#             data_collator=data_collator\n",
        "#         )\n",
        "#         trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Save the final updated model\n",
        "# model.save_pretrained(OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOzOu1u5y0OC"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDlFyd8wA-pY"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from peft import PeftModel\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
        "# from transformers import pipeline\n",
        "\n",
        "# model_name = \"decapoda-research/llama-7b-hf\"\n",
        "# adapters_name = 'haraygese/llama-7b-for-custom-data'\n",
        "\n",
        "# print(f\"Starting to load the model {model_name} into memory\")\n",
        "\n",
        "# m = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_name,\n",
        "#     #load_in_4bit=True,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map={\"\": 0}\n",
        "# )\n",
        "# m = PeftModel.from_pretrained(m, adapters_name)\n",
        "# m = m.merge_and_unload()\n",
        "# tok = LlamaTokenizer.from_pretrained(model_name)\n",
        "# tok.bos_token_id = 1\n",
        "\n",
        "# stop_token_ids = [0]\n",
        "\n",
        "# print(f\"Successfully loaded the model {model_name} into memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41_9nmPVJDz5"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAGpTrWLBIXc"
      },
      "outputs": [],
      "source": [
        "# from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "# from pydantic import Extra, root_validator\n",
        "\n",
        "# from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "# from langchain.llms.base import LLM\n",
        "# from langchain.llms.utils import enforce_stop_tokens\n",
        "\n",
        "# from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# class HuggingFaceHugs(LLM):\n",
        "#   pipeline: Any\n",
        "#   class Config:\n",
        "#     \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "#     extra = Extra.forbid\n",
        "\n",
        "#   def __init__(self, model, tokenizer, task=\"text-generation\"):\n",
        "#     super().__init__()\n",
        "#     self.pipeline = pipeline(task, model=model, tokenizer=tokenizer)\n",
        "\n",
        "#   @property\n",
        "#   def _llm_type(self) -> str:\n",
        "#     \"\"\"Return type of llm.\"\"\"\n",
        "#     return \"huggingface_hub\"\n",
        "\n",
        "#   def _call(self, prompt, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None,):\n",
        "#     # Runt the inference.\n",
        "#     text = self.pipeline(prompt, max_length=100)[0]['generated_text']\n",
        "\n",
        "#     # @alvas: I've totally no idea what this in langchain does, so I copied it verbatim.\n",
        "#     if stop is not None:\n",
        "#       # This is a bit hacky, but I can't figure out a better way to enforce\n",
        "#       # stop tokens when making calls to huggingface_hub.\n",
        "#       text = enforce_stop_tokens(text, stop)\n",
        "#     print(text)\n",
        "#     return text[len(prompt):]\n",
        "\n",
        "\n",
        "# template = \"\"\" Hey llama, you like to eat quinoa. Whatever question I ask you, you reply with \"Waffles, waffles, waffles!\".\n",
        "#  Question: {input} Answer: \"\"\"\n",
        "# prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
        "\n",
        "\n",
        "# hf_model = HuggingFaceHugs(model=m, tokenizer=tok)\n",
        "\n",
        "# chain = LLMChain(prompt=prompt, llm=hf_model)\n",
        "\n",
        "# chain(\"PACT is a US-based brand that is\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0676264d73554a53a69aa586c9451b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1a08049d3a04ffc940574dbbdceda93",
            "placeholder": "​",
            "style": "IPY_MODEL_d85fb18568e3470e97e2fba40a0054a0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "08a7fae4d4604dbd8b387f89e3f6f7d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1ca9b2b5c64ac89240bfe40403123a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12aee97766494b0b899bcbf7c2959253",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d0b36eb8ef4037a28375829c1649b0",
            "value": " 33/33 [01:56&lt;00:00,  4.26s/it]"
          }
        },
        "0cc3f46b77784e7d891c34594bfbc74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12aee97766494b0b899bcbf7c2959253": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c2af134099f45bab077ccfd461b8f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08a7fae4d4604dbd8b387f89e3f6f7d4",
            "placeholder": "​",
            "style": "IPY_MODEL_f172f02fe79e42f5b74a135493861bc6",
            "value": "Map: 100%"
          }
        },
        "2de6c494dd954beaaae9b15d319e271c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a045c6b29804b66b58e06fcdfe3cb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c2af134099f45bab077ccfd461b8f31",
              "IPY_MODEL_ff68e9fdc6994d06ba6eccc27ef9facb",
              "IPY_MODEL_792bb58c57b846d3a20243ab349eefd3"
            ],
            "layout": "IPY_MODEL_5b13f3fd744d4e698f8769864b5c1a44"
          }
        },
        "5b13f3fd744d4e698f8769864b5c1a44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "792bb58c57b846d3a20243ab349eefd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f8c9d7a7c4467da5484feb56764ad1",
            "placeholder": "​",
            "style": "IPY_MODEL_2de6c494dd954beaaae9b15d319e271c",
            "value": " 283/284 [00:00&lt;00:00, 864.35 examples/s]"
          }
        },
        "86179f9e043e40ed8048686b40b2ff85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebe3454387f4aa5a81b13316b65c62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0676264d73554a53a69aa586c9451b81",
              "IPY_MODEL_e8eba6c1a6064b41b197e69fbd080a0b",
              "IPY_MODEL_0c1ca9b2b5c64ac89240bfe40403123a"
            ],
            "layout": "IPY_MODEL_9c6a551642be4b0ebce3a8646cd12921"
          }
        },
        "905583213b0f438aa0c225e5d8767445": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c6a551642be4b0ebce3a8646cd12921": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6362da7503492db08aa174f6478e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4f8c9d7a7c4467da5484feb56764ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d85fb18568e3470e97e2fba40a0054a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8eba6c1a6064b41b197e69fbd080a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_905583213b0f438aa0c225e5d8767445",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cc3f46b77784e7d891c34594bfbc74b",
            "value": 33
          }
        },
        "f172f02fe79e42f5b74a135493861bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1a08049d3a04ffc940574dbbdceda93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d0b36eb8ef4037a28375829c1649b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff68e9fdc6994d06ba6eccc27ef9facb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86179f9e043e40ed8048686b40b2ff85",
            "max": 284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af6362da7503492db08aa174f6478e38",
            "value": 284
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}